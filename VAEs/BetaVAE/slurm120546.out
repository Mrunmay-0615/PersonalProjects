==========================================
SLURM_CLUSTER_NAME = paramganga
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = iitr
SLURM_JOB_ID = 120546
SLURM_JOB_NAME = betavae
SLURM_JOB_NODELIST = gpu005
SLURM_JOB_USER = angetkar_mk.iitr
SLURM_JOB_UID = 38418
SLURM_JOB_PARTITION = gpu
SLURM_TASK_PID = 22180
SLURM_SUBMIT_DIR = /home/angetkar_mk.iitr/Mrunmay/PyTorch-VAE
SLURM_CPUS_ON_NODE = 10
SLURM_NTASKS = 10
SLURM_TASK_PID = 22180
==========================================
no change     /home/apps/DL/DL-CondaPy3.7/condabin/conda
no change     /home/apps/DL/DL-CondaPy3.7/bin/conda
no change     /home/apps/DL/DL-CondaPy3.7/bin/conda-env
no change     /home/apps/DL/DL-CondaPy3.7/bin/activate
no change     /home/apps/DL/DL-CondaPy3.7/bin/deactivate
no change     /home/apps/DL/DL-CondaPy3.7/etc/profile.d/conda.sh
no change     /home/apps/DL/DL-CondaPy3.7/etc/fish/conf.d/conda.fish
no change     /home/apps/DL/DL-CondaPy3.7/shell/condabin/Conda.psm1
no change     /home/apps/DL/DL-CondaPy3.7/shell/condabin/conda-hook.ps1
no change     /home/apps/DL/DL-CondaPy3.7/lib/python3.7/site-packages/xontrib/conda.xsh
no change     /home/apps/DL/DL-CondaPy3.7/etc/profile.d/conda.csh
no change     /home/angetkar_mk.iitr/.bashrc
No action taken.
Global seed set to 1265
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Global seed set to 1265
initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

======= Training BetaVAE =======
Traceback (most recent call last):
  File "/home/angetkar_mk.iitr/Mrunmay/PyTorch-VAE/run.py", line 62, in <module>
    runner.fit(experiment, datamodule=data)
  File "/home/angetkar_mk.iitr/.conda/envs/betavae/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 737, in fit
    self._call_and_handle_interrupt(
  File "/home/angetkar_mk.iitr/.conda/envs/betavae/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 682, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/angetkar_mk.iitr/.conda/envs/betavae/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 772, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/angetkar_mk.iitr/.conda/envs/betavae/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1133, in _run
    self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment
    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/angetkar_mk.iitr/.conda/envs/betavae/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1433, in _call_setup_hook
    self.datamodule.setup(stage=fn)
  File "/home/angetkar_mk.iitr/.conda/envs/betavae/lib/python3.11/site-packages/pytorch_lightning/core/datamodule.py", line 461, in wrapped_fn
    has_run = getattr(obj, attr)
              ^^^^^^^^^^^^^^^^^^
AttributeError: 'VAEDataset' object has no attribute '_has_setup_TrainerFn.FITTING'
